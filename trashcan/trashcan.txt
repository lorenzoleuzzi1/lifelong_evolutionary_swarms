def step(self, action):
        
        self._rewards = np.zeros(self.n_agents)

        for i in range(self.n_agents):
            
            if action[i]['action'] == RANDOM_WALK:
                # Map the action to the direction we walk in
                direction = self.np_random.uniform(0, 360)
                distance = self.np_random.uniform(0, self.max_distance_covered_per_step)
                
                # Calculate the new position
                direction_radians = np.radians(direction)
                dx = round(distance * np.cos(direction_radians), 5)
                dy = round(distance * np.sin(direction_radians), 5)
                # `np.clip` to make sure we don't leave the grid
                new_x = np.clip(self.agents_location[i][0] + dx, 0, self.size - 1)
                new_y = np.clip(self.agents_location[i][1] + dy, 0, self.size - 1)
                new_position = np.array([new_x, new_y])
                self.agents_location[i] = new_position
            
            self._rewards[i] += REWARD_MOVE # Punish the agent for moving for enforce efficiency

            if action[i]['action'] == PICK:
                successful_pick = False
                if self._agents_picked_up[i] == -1: # If the agent is not carrying a block
                    for j in range(self.n_blocks):
                        # If the agent is in the same location as the block
                        if np.linalg.norm(self.agents_location[i] - self.blocks_location[j]) < self.sensitivity: 
                            successful_pick = True
                            self.blocks_location[j] = [-1,-1] # Set as picked up
                            self._agents_picked_up[i] = self.blocks_color[j] # The agent knows the color of the block it picked up
                            self._blocks_picked_up[j] = i # The block is picked up by the agent

                            # Reward the agent for picking up the block if in objective or not
                            if self.blocks_color[j] in self._objective_colors:
                                self._rewards[i] += REWARD_CORRECT_PICK
                            else:
                                self._rewards[i] += REWARD_WRONG_PICK
                            break
                if not successful_pick:
                    self._rewards[i] += REWARD_MOVE # Punish the agent for picking nothing

            
            if action[i]['action'] == DROP:
                
                else:
                    self._rewards[i] += REWARD_MOVE # Punish the agent for dropping nothing

            if action[i]['action'] == MOVE:
                # Map the action to the direction we walk in
                distance, direction = action[i]['move']
                
                # Calculate the new position
                direction_radians = direction
                if direction > 180:
                    direction_radians -= 360
                direction_radians = np.radians(direction)
                dx = round(distance * np.cos(direction_radians), 5)
                dy = round(distance * np.sin(direction_radians), 5)
                # `np.clip` to make sure we don't leave the grid
                new_x = np.clip(self.agents_location[i][0] + dx, 0, self.size - 1)
                new_y = np.clip(self.agents_location[i][1] + dy, 0, self.size - 1)
                new_position = np.array([new_x, new_y])

                # Check if the new position is not too close to another agent
                agent_locations_but_i = np.delete(self.agents_location, i, axis=0)
                differences = new_position - agent_locations_but_i
                distances = np.linalg.norm(differences, axis=1)
                occupied_by_agent = np.any(distances < self.sensitivity)
                # Check if the new position is not too close to a block while carrying one (can't pick up two blocks)
                occupied_by_block_while_carrying = np.any(np.linalg.norm(new_position - self.blocks_location, axis=1) < self.sensitivity) and self._agents_picked_up[i] != -1
                # Same poisition as before considering sensitvity
                same_position = np.linalg.norm(new_position - self.agents_location[i]) < self.sensitivity
                
                # Move the agent if the new position is not occupied by another agent or a block while carrying
                if not occupied_by_agent and not occupied_by_block_while_carrying and not same_position:
                    self.agents_location[i] = new_position

                    # Reward the agent for moving toward the detected objects
                    if self._agents_picked_up[i] == -1: # If the agent is not carrying a block
                        for neighbor in self._neighbors[i]:
                            if neighbor[0] in self._objective_colors:
                                direction_difference_object = abs(direction - neighbor[2]) # Calculate the absolute difference
                                direction_difference_object = min(direction_difference_object, 360 - direction_difference_object) # Adjust for cases where the difference is more than 180°
                                direction_difference_object = max(1 - (direction_difference_object / 45), 0)  # Normalized to [0, 1]
                                
                                self._rewards[i] += REWARD_CORRECT_MOVE * direction_difference_object
                    
                    # Reward the agent to have moved near the objective (so it can pick it up)
                    if self._agents_picked_up[i] == -1: # If the agent is not carrying a block
                        for j in range(self.n_blocks):
                            if np.linalg.norm(self.agents_location[i] - self.blocks_location[j]) < self.sensitivity:
                                if self.blocks_color[j] in self._objective_colors:
                                    self._rewards[i] += REWARD_CORRECT_MOVE
                                break
                    
                    # # Punish the agent for moving away the objective
                    # if self._agents_picked_up[i] == -1:
                    #     for j in range(self.n_blocks):
                    #         if np.linalg.norm(self.agents_location[i] - self.blocks_location[j]) < self.sensitivity:
                    #             if self.blocks_color[j] not in self._objective_colors:
                    #                 self._rewards[i] += REWARD_WRONG_MOVE
                    #             break
                    # TODO: penalize collisions with other agents, walls, and blocks
                    # Reward the agent for moving in the right direction while carrying
                    if self._agents_picked_up[i] in self._objective_colors: # If the agent is carrying a objective block
                        target_edge = self.objective[self._objective_colors.index(self._agents_picked_up[i])][1]
                        
                        direction_difference_edge = abs(direction - target_edge) # Calculate the absolute difference
                        direction_difference_edge = min(direction_difference_edge, 360 - direction_difference_edge) # Adjust for cases where the difference is more than 180°
                        direction_difference_edge = max(1 - (direction_difference_edge / 45), 0)  # Normalized to [0, 1]

                        self._rewards[i] += REWARD_CORRECT_MOVE * distance * direction_difference_edge 
        
        self._detect()
        observations = []
        for i in range(self.n_agents):
            observation = self._get_obs(i)

            # for neighbor in observation['neighbors']:
            #     if neighbor[0] in self._objective_colors:
            #         self._rewards[i] +=  REWARD_DETECTING - neighbor[1] # Reward the agent for moving towards the objective
            
            observations.append(observation)
        
        done = False
        # Check if the shared objective is met
        if len(self._completed) == self._task_counter:
            done = True

        reward = sum(self._rewards) # Sum the rewards of all agents of the swarm
        info = {"completed": self._completed}
        truncated = False
        
        return observations, reward, done, truncated, info
    





    no_detections = []
            for o in obs:
                # If it has no neighbors and it is not carrying a block
                no_detections.append(np.all(o['neighbors'][0] == 0) and o['carrying'] == -1)
            
            random_actions = env.action_space.sample()
            if (len(no_detections) - sum(no_detections)) != env.n_agents: # If there is at least one agent with neighbors
                nn_inputs = env.process_observation(obs)
                nn_outputs = np.array(network.predict(nn_inputs))
                actions = np.round(nn_outputs * np.array([env.max_distance_covered_per_step, env.sensor_angle]), 1)
                
                for i in range(env.n_agents):
                    if no_detections[i]:
                        actions[i]= random_actions[i]
            else:
                actions = random_actions

            obs, reward, done, _, _ = env.step(actions)


def _calculate_distance_direction(self, pointA, pointB, distance_type='euclidean', only_distance=False):
        x1, y1 = pointA
        x2, y2 = pointB

        # Calculate distance
        if distance_type == 'manhattan':
            distance = abs(x1 - x2) + abs(y1 - y2)
        elif distance_type == 'euclidean':
            # distance = sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)
            distance = np.linalg.norm(pointA - pointB)
        else:
            raise ValueError("Invalid distance type. Use 'manhattan' or 'euclidean'.")

        # Calculate direction in degrees
        angle_radians = atan2(y2 - y1, x2 - x1)
        direction_degrees = degrees(angle_radians)

        # Normalize the direction to be between 0 and 360 degrees
        if direction_degrees < 0:
            direction_degrees += 360
        # S is 0/360 degrees, E is 90 degrees, N is 180 degrees, W is 270 degrees

        return distance, direction_degrees


        self._rewards = np.zeros(self.n_agents)
        


        # Reset sensors
        self._previous_neighbors = self._neighbors.copy()
        # self._neighbors = np.zeros((self.n_agents, self.n_neighbors, 3), dtype=float)
        
        for i in range(self.n_agents):
            # Get the action of the agent
            distance, direction = action[i]
            if distance > self.max_distance_covered_per_step:
                distance = self.max_distance_covered_per_step
            
            # Calculate the new position
            direction_radians = direction
            if direction > 180:
                direction_radians -= 360
            direction_radians = np.radians(direction)
            dx = round(distance * np.cos(direction_radians), 2)
            dy = round(distance * np.sin(direction_radians), 2)
            # `np.clip` to make sure we don't leave the grid
            new_x = self.agents_location[i][0] + dx
            new_y = self.agents_location[i][1] + dy
            new_position = np.array([new_x, new_y])
            # # Check if it is trying to leave the arena
            # if np.any(new_position < 0) or np.any(new_position >= self.size):
            #     self._rewards[i] += REWARD_WRONG_MOVE # Penalize the agent for crushing into wall
            # Clip values to stay within the grid
            new_position = np.clip(new_position, 0, self.size - 1)
            
            # TODO: this checks or penalize with reward system to avoid these
            # Check if the new position is not too close to another agent
            agent_locations_but_i = np.delete(self.agents_location, i, axis=0)
            differences = new_position - agent_locations_but_i
            distances = np.linalg.norm(differences, axis=1)
            occupied_by_agent = np.any(distances < self.sensitivity)
            # Check if the new position is not too close to a block while carrying one (can't pick up two blocks)
            occupied_by_block_while_carrying = np.any(np.linalg.norm(new_position - self.blocks_location, axis=1) < self.sensitivity) and self._agents_carrying[i] != -1
    


    single_action_space = spaces.Box(low=np.array([0, 0]), 
                                         high=np.array([max_distance_covered_per_step, sensor_angle]), dtype=float)





# Choose the location at random
            for i in range(self.n_agents):
                # Check if the agents are not spawning in the same location
                while True:
                    # Spawn the agents in the nest
                    if self.nest == UP:
                        self.agents_location[i][0] = 0
                        self.agents_location[i][1] = self.np_random.uniform(0, self.size)
                    elif self.nest == DOWN:
                        self.agents_location[i][0] = self.size - 1
                        self.agents_location[i][1] = self.np_random.uniform(0, self.size)
                    elif self.nest == LEFT:
                        self.agents_location[i][0] = self.np_random.uniform(0, self.size)
                        self.agents_location[i][1] = 0
                    elif self.nest == RIGHT:
                        self.agents_location[i][0] = self.np_random.uniform(0, self.size)
                        self.agents_location[i][1] = self.size - 1
                    if i == 0 or not np.any(np.linalg.norm(self.agents_location[i] - self.agents_location[:i], axis=1) < self.sensitivity):
                        break

            for i in range(self.n_blocks - 1):
                # Check if the blocks are not spawning in the same location
                while True:
                    self.blocks_location[i] = self.np_random.uniform(2, self.size - 2, size=2)
                    if i == 0 or not np.any(np.linalg.norm(self.blocks_location[i] - self.blocks_location[:i], axis=1) < self.sensitivity):
                        break
                self.blocks_color[i] = self.np_random.integers(3, 3 + len(self._colors_map), dtype=int)
            # At least one block is objective
            self.blocks_location[self.n_blocks - 1] = self.np_random.uniform(2, self.size - 2, size=2)
            self.blocks_color[self.n_blocks - 1] = self.objective[0][0]



            if initial_setting is not None:
            # TODO: add these checks
            assert 'agents' in initial_setting, "Initial setting must contain 'agents'"
            assert 'blocks' in initial_setting, "Initial setting must contain 'blocks'"
            assert 'colors' in initial_setting, "Initial setting must contain 'colors'"

            self.n_agents = len(initial_setting['agents'])
            self.n_blocks = len(initial_setting['blocks'])
            
            # Check if agents are not in the same location
            for i in range(self.n_agents ):
                for j in range(self.n_agents ):
                    if i != j:
                        assert not np.all(initial_setting['agents'][i] == initial_setting['agents'][j]), f"Agent {i} and agent {j} are spawning in the same location"
            # Check if agents and blocks are not in the same location
            for i in range(self.n_agents ):
                for j in range(self.n_blocks ):
                    assert not np.all(initial_setting['agents'][i] == initial_setting['blocks'][j]), f"Agent {i} and block {j} are spawning in the same location"
            
            # Check if blocks are not spawning in the same location
            for i in range(self.n_blocks ):
                for j in range(self.n_blocks):
                    if i != j:
                        assert not np.all(initial_setting['blocks'][i] == initial_setting['blocks'][j]), f"Block {i} and block {j} are spawning in the same location"
            
            # Check if agents and blocks are within the arena
            for agent in initial_setting['agents']:
                assert agent[0] >= 0 and agent[0] < size, f"Agent {agent} is outside the arena"
                assert agent[1] >= 0 and agent[1] < size, f"Agent {agent} is outside the arena"
            for block in initial_setting['blocks']:
                assert block[0] >= 0 and block[0] < size, f"Block {block} is outside the arena"
                assert block[1] >= 0 and block[1] < size, f"Block {block} is outside the arena"
            
            # All the color objective must be in the blocks colors
            for color, _ in objective:
                assert color in initial_setting['colors'], f"Color {color} in objective is not present in the arena"



    
EASY_INITIAL_SETTING = {
            'agents': np.array([[0, 5], [0, 10], [0, 15]], dtype=float),
            'headings': np.array([DOWN, DOWN, DOWN], dtype=float), 
            'blocks': np.array([[9, 16], [10, 12], [11, 6]], dtype=float),
            'colors': np.array([RED, GREEN, RED], dtype=int)
            }

MEDIUM_INITIAL_SETTING = {
            'agents': np.array([[0, 5], [0, 10], [0, 15]], dtype=float),
            'headings': np.array([DOWN, DOWN, DOWN], dtype=float),
            'blocks': np.array([[9, 16], [11, 5], [6, 5], [10, 11], [8, 8]], dtype=float),
            'colors': np.array([RED, RED, BLUE, GREEN, RED], dtype=int)
            }

HARD_INITIAL_SETTING = {
            'agents': np.array([[0, 3], [0, 6], [0, 9], [0, 12], [0, 15]], dtype=float),
            'headings': np.array([DOWN, DOWN, DOWN, DOWN, DOWN], dtype=float),
            'blocks': np.array([[9, 16], [13, 7], [11, 3], [10, 11], [9, 7], [16, 8], [15, 17],
                               [7, 5], [15, 3], [11, 11], [8, 12], [15, 13], [7, 4]], dtype=float),
            'colors': np.array([RED, RED, RED, RED, RED, RED, RED, RED, BLUE, BLUE, 
                                GREEN, YELLOW, PURPLE], dtype=int)
            }


if self.nest == UP:
                    low = (0, 0)
                    high = (3, SIMULATION_ARENA_SIZE)
                    heading = DOWN
                elif self.nest == DOWN:
                    low = (SIMULATION_ARENA_SIZE - 3, 0)
                    high = (SIMULATION_ARENA_SIZE, SIMULATION_ARENA_SIZE)
                    heading = UP
                elif self.nest == LEFT:
                    low = (0, 0)
                    high = (SIMULATION_ARENA_SIZE, 3)
                    heading = RIGHT
                elif self.nest == RIGHT:
                    low = (0, SIMULATION_ARENA_SIZE - 3)
                    high = (3, SIMULATION_ARENA_SIZE)
                    heading = LEFT


                        def _is_in_drop_zone(self, agent_position):
    def _is_close_to_edge(self, agent_position):
        if agent_position[0] < 1 or agent_position[0] > self.size - 2:
            return True
        if agent_position[1] < 1 or agent_position[1] > self.size - 2:
            return True
        return False

    def _is_correct_drop(self, agent_position, block_idx):
        return agent_position[0] < 1 and self.blocks_color[block_idx] == self.target_color


            # # Handle collisions
        # distances = np.linalg.norm(self.agents_location[:, np.newaxis] - self.agents_location, axis=2)
        # collision_matrix = (distances < self.sensitivity) & (distances > 0)
        # colliding_agents = np.unique(np.where(collision_matrix)[0])
        # if colliding_agents.size > 0:
        #     # self._rewards[colliding_agents] += REWARD_COLLISION  # Penalize collision
        #     # If collision, rotate at max speed
        #     self.agents_location[colliding_agents] = old_agents_location[colliding_agents]
        #     omega_c = self.max_wheel_velocity / (ROBOT_SIZE / 2)
        #     self.agents_heading[colliding_agents] = np.mod(
        #         old_agents_heading[colliding_agents] + np.degrees(omega_c * self.time_step), 360)




#     if self.regularization_retaining == "gd" or self.regularization_retaining == "genetic_distance":
        #             regularization_str = "_gd_"
        #     elif self.regularization_retaining == "wp" or self.regularization_retaining == "weight_protection":
        #             regularization_str = "wp"
        #     if regularization_str == "_gd_":
        #         lambdas_str = f"_{self.reg_lambdas.get('gd')}"
        #     elif regularization_str == "wp":
        #             lambdas_str = f"_{self.reg_lambdas.get('wp')[0]}_{self.reg_lambdas.get('wp')[1]}"
        # else:
        #     regularization_str = ""
        #     lambdas_str = ""






        # Regular expression pattern to match the desired part
    # pattern = r"^(?!.*drift).*$"

    # # Set to store unique parts of the filenames
    # unique_parts = set()

    # # Iterate through all files in the specified directory
    # for filename in os.listdir(results_path):
    #     # Check if the filename matches the pattern
    #     match = re.match(pattern, filename)
    #     if match:
    #         # If there's a match, add it to the set
    #         unique_parts.add(match.group()[:-2]) # remove the seed TODO: if seed is longer that 1 digit dont work

    # # Convert the set to a sorted list
    # exps = sorted(list(unique_parts))

    # # Print the result
    # print(exps)

    # exps = ["baselinefrgd3_neat_500_500_50_3_7_b", "baselinefrgd3_neat_500_500_50_3_7_u"]
    # path = "/Users/lorenzoleuzzi/Library/CloudStorage/OneDrive-UniversityofPisa/lifelong\ evolutionary\ swarms/results" 
    # path = "results"









        # drift_str = ""
        # for i, task in enumerate(tasks):
        #     experiments = []
        #     if i != 0:
        #         drift_str = drift_str + f"_drift{tasks[i-1]}{tasks[i]}"
        #     for seed in seeds:
        #         if i == 0:
        #             experiments.append(f"{path}/{name}_{seed}")
        #         else:
        #             experiments.append(f"{path}/{name}_{seed}{drift_str}")
        #     experiments_list.append(experiments)
        
        # plot_evolutions_and_drifts(experiments_list, tasks, name)
    
    
    # experiments_list = [[
    #     f"{results_path}/gdcoarse5_neat_800_100_100_5_30_u_1",
    #     # f"{results_path}/bristol/bristol2gd3_neat_500_50_300_3_7_u_0",
    #     # f"{results_path}/bristol/bristol3gd3_neat_500_50_300_3_7_u_0"
    #     # f"{results_path}/bristol/gd_3seed3gd3_neat_500_50_300_3_7_u_3",
    #     # f"{results_path}/bristol/gd_3seed4gd3_neat_500_50_300_3_7_u_3",
    #     # f"{results_path}/bristol/gd_3seed5gd3_neat_500_50_300_3_7_u_3",
    #     # f"{results_path}/bristol/gd_3seed6gd3_neat_500_50_300_3_7_u_3",
    #     # f"{results_path}/bristol/gd_3seed7gd3_neat_500_50_300_3_7_u_3",
    #     # f"{results_path}/bristol/gd_3seed8gd3_neat_500_50_300_3_7_u_3",
    #     # f"{results_path}/bristol/gd_3seed9gd3_neat_500_50_300_3_7_u_3"
    # ],
    #     [
    #     f"{results_path}/gdcoarse21_neat_800_100_100_5_30_u_1_drift34",
    #     # f"{results_path}/bristol/bristol2gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/bristol3gd3_neat_500_50_300_3_7_u_0_drift34"
    #     # f"{results_path}/bristol/gd_3seed3gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/gd_3seed4gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/gd_3seed5gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/gd_3seed6gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/gd_3seed7gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/gd_3seed8gd3_neat_500_50_300_3_7_u_0_drift34",
    #     # f"{results_path}/bristol/gd_3seed9gd3_neat_500_50_300_3_7_u_0_drift34"
    # ],
    # [
    #     f"{results_path}/gdcoarse5_neat_800_100_100_5_30_u_5_drift34_drift43",
    #     # f"{results_path}/bristol/bristol2gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/bristol3gd3_neat_500_50_300_3_7_u_0_drift34_drift43"
    #     # f"{results_path}/bristol/gd_3seed3gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/gd_3seed4gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/gd_3seed5gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/gd_3seed6gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/gd_3seed7gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/gd_3seed8gd3_neat_500_50_300_3_7_u_0_drift34_drift43",
    #     # f"{results_path}/bristol/gd_3seed9gd3_neat_500_50_300_3_7_u_0_drift34_drift43"
    # ]      
    # ]
              





              # PLOTTING

    # baf = np.array(baf).flatten()
    # print(f"Best Anytime Fitness: {np.mean(baf)}")
    # print(f"Average End Evolution Fitness: {np.mean(beef)}")
    # for i in range(len(best_instances)):
    #     print(f"Best seed for evo {i} at {best_instances[i]}") 
    
    # raf = np.array(raf).flatten()
    # print(f"Average Retaining End Evolution Fitness: {np.mean(reef)}")
    # for i in range(len(best_retention_instances)):
    #     print(f"Best seed for retention evo {i} at {best_retention_instances[i]}")




    
            

    # best_instances = []
    # best_retention_instances = []
    # best_instances_fitness = []
    # best_retention_instances_fitness = []

    # plotted_bests = []
    # plotted_retentions = []


    # for i, drift in enumerate(drifts):
        

    #     # Iterate thru the drifts
    #     log_drift = []
    #     experiment_info_drift = []
    #     baf_drift = []
    #     beef_drift = []
    #     raf_drift = []
    #     reef_drift = []
    #     # ----- LOAD INFO & LOG -----
    #     for j in range(n_seeds):
    #         # Iterates thru the experiments instances (seeds)
    #         log = load_logbook_json(drifts[drift][j])
    #         log_drift.append(log)
    #         experiment_info = load_experiment_json(drifts[drift][j])
    #         experiment_info_drift.append(experiment_info)

    #         if j != 0:
    #             if check_generations != experiment_info["generations"]:
    #                 raise ValueError(f"Duration mismatch: {check_generations} != {experiment_info["generations"]}. 
    #                                  Experiment must have the same number of generations.")
    #         check_generations= experiment_info["generations"]

    #         # Metrics
    #     # ---------------------------

    #     current_gen = counter_gens
    #     counter_gens += len(log_drift[0]["best"])

    #     # ----- RETENTION -----
    #     if i != 0:
    #         type_of_retention = experiment_info_drift[0]["type_of_retention"]

    #         prev_task = task
    #         prev_color = tasks_plot_color_map[prev_task]
            
    #         raf = [log["retention"] for log in log_drift]
    #         retention_curve = [log["retention"] for log in log_drift]
    #         raf.append(np.mean(retention_curve, axis=0))
    #         # avg_retention = np.mean(retention, axis=0)

    #         # best_retention_instances.append(np.argmax([experiment_info[f"best_fitness_retention_{type_of_retention}"] for experiment_info in experiment_info_drift]))
    #         # best_retention_instances_fitness.append(np.max([experiment_info[f"best_fitness_retention_{type_of_retention}"] for experiment_info in experiment_info_drift]))
            
    #         # TODO: not needed if retention is same of the plot, also can be retaing find bests
    #         # retention_end = [experiment_info[f"best_fitness_retention_{type_of_retention}"] for experiment_info in experiment_info_drift]
    #         # avg_retention_end = np.mean(retention_end)
            
    #         # raf.extend(avg_retention)
    #         # reef.append(avg_retention_end)
            
    #         # Range from current to counter_gens but every 10 gens
    #         range_retention = np.arange(current_gen, counter_gens, 10) 
            
    #         # Plot drifts lines
    #         if i == 1:
    #             plt.axvline(x=current_gen, linestyle = '--', color='grey', label=f"Drift")
    #         else:    
    #             plt.axvline(x=current_gen, linestyle = '--', color='grey')
            
    #         if prev_color not in plotted_retentions:
    #             plt.plot(range_retention, avg_retention, linestyle='-.', label=f"Retention {type_of_retention} ({prev_color})", color=prev_color)
    #             plotted_retentions.append(prev_color)
    #         else:
    #             plt.plot(range_retention, avg_retention, linestyle='-.', color=prev_color)
            
    #         for retention in retention:
    #             plt.plot(range_retention, retention, linestyle='-.', color=prev_color, alpha=0.2)

        
    #         # Forgetting
    #         forgetting = best_end - retention_end
    #         avg_forgetting.append(np.mean(forgetting))

    #     # ---------------------

    #     # ----- BEST -----
    #     task = int(drift[-1])
    #     current_color = tasks_plot_color_map[task]
        
    #     bests = []
        
    #     # Use the not penalized fitness when applying regularization
    #     for log in log_drift:
    #         no_penalty = log.get("no_penalty", [])
    #         if no_penalty == []:
    #             bests.append(log["best"]) 
    #         else:
    #             bests.append(no_penalty)
                
    #     avg_bests = np.mean(bests, axis=0)
    #     baf.extend(avg_bests)
        
    #     best_end = [experiment_info["best_fitness"] for experiment_info in experiment_info_drift]
    #     beef.append(np.mean(best_end))
        
    #     best_instances.append(np.argmax([experiment_info["best_fitness"] for experiment_info in experiment_info_drift]))
    #     best_instances_fitness.append(np.max([experiment_info["best_fitness"] for experiment_info in experiment_info_drift]))

        
    #     # Plot best
    #     # Check if the task was already plotted
    #     if current_color not in plotted_bests:
    #         plt.plot(range(current_gen, counter_gens), avg_bests, color=current_color, label=f"Best ({current_color})")
    #         plotted_bests.append(current_color)
    #     else:
    #         plt.plot(range(current_gen, counter_gens), avg_bests, color=current_color)

    #     for best in bests:
    #         plt.plot(range(current_gen, counter_gens), best, color=current_color, alpha=0.2)
        
        # ---------------- 
