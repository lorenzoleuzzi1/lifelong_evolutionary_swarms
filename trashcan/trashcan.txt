def step(self, action):
        
        self._rewards = np.zeros(self.n_agents)

        for i in range(self.n_agents):
            
            if action[i]['action'] == RANDOM_WALK:
                # Map the action to the direction we walk in
                direction = self.np_random.uniform(0, 360)
                distance = self.np_random.uniform(0, self.max_distance_covered_per_step)
                
                # Calculate the new position
                direction_radians = np.radians(direction)
                dx = round(distance * np.cos(direction_radians), 5)
                dy = round(distance * np.sin(direction_radians), 5)
                # `np.clip` to make sure we don't leave the grid
                new_x = np.clip(self.agents_location[i][0] + dx, 0, self.size - 1)
                new_y = np.clip(self.agents_location[i][1] + dy, 0, self.size - 1)
                new_position = np.array([new_x, new_y])
                self.agents_location[i] = new_position
            
            self._rewards[i] += REWARD_MOVE # Punish the agent for moving for enforce efficiency

            if action[i]['action'] == PICK:
                successful_pick = False
                if self._agents_picked_up[i] == -1: # If the agent is not carrying a block
                    for j in range(self.n_blocks):
                        # If the agent is in the same location as the block
                        if np.linalg.norm(self.agents_location[i] - self.blocks_location[j]) < self.sensitivity: 
                            successful_pick = True
                            self.blocks_location[j] = [-1,-1] # Set as picked up
                            self._agents_picked_up[i] = self.blocks_color[j] # The agent knows the color of the block it picked up
                            self._blocks_picked_up[j] = i # The block is picked up by the agent

                            # Reward the agent for picking up the block if in objective or not
                            if self.blocks_color[j] in self._objective_colors:
                                self._rewards[i] += REWARD_CORRECT_PICK
                            else:
                                self._rewards[i] += REWARD_WRONG_PICK
                            break
                if not successful_pick:
                    self._rewards[i] += REWARD_MOVE # Punish the agent for picking nothing

            
            if action[i]['action'] == DROP:
                
                else:
                    self._rewards[i] += REWARD_MOVE # Punish the agent for dropping nothing

            if action[i]['action'] == MOVE:
                # Map the action to the direction we walk in
                distance, direction = action[i]['move']
                
                # Calculate the new position
                direction_radians = direction
                if direction > 180:
                    direction_radians -= 360
                direction_radians = np.radians(direction)
                dx = round(distance * np.cos(direction_radians), 5)
                dy = round(distance * np.sin(direction_radians), 5)
                # `np.clip` to make sure we don't leave the grid
                new_x = np.clip(self.agents_location[i][0] + dx, 0, self.size - 1)
                new_y = np.clip(self.agents_location[i][1] + dy, 0, self.size - 1)
                new_position = np.array([new_x, new_y])

                # Check if the new position is not too close to another agent
                agent_locations_but_i = np.delete(self.agents_location, i, axis=0)
                differences = new_position - agent_locations_but_i
                distances = np.linalg.norm(differences, axis=1)
                occupied_by_agent = np.any(distances < self.sensitivity)
                # Check if the new position is not too close to a block while carrying one (can't pick up two blocks)
                occupied_by_block_while_carrying = np.any(np.linalg.norm(new_position - self.blocks_location, axis=1) < self.sensitivity) and self._agents_picked_up[i] != -1
                # Same poisition as before considering sensitvity
                same_position = np.linalg.norm(new_position - self.agents_location[i]) < self.sensitivity
                
                # Move the agent if the new position is not occupied by another agent or a block while carrying
                if not occupied_by_agent and not occupied_by_block_while_carrying and not same_position:
                    self.agents_location[i] = new_position

                    # Reward the agent for moving toward the detected objects
                    if self._agents_picked_up[i] == -1: # If the agent is not carrying a block
                        for neighbor in self._neighbors[i]:
                            if neighbor[0] in self._objective_colors:
                                direction_difference_object = abs(direction - neighbor[2]) # Calculate the absolute difference
                                direction_difference_object = min(direction_difference_object, 360 - direction_difference_object) # Adjust for cases where the difference is more than 180°
                                direction_difference_object = max(1 - (direction_difference_object / 45), 0)  # Normalized to [0, 1]
                                
                                self._rewards[i] += REWARD_CORRECT_MOVE * direction_difference_object
                    
                    # Reward the agent to have moved near the objective (so it can pick it up)
                    if self._agents_picked_up[i] == -1: # If the agent is not carrying a block
                        for j in range(self.n_blocks):
                            if np.linalg.norm(self.agents_location[i] - self.blocks_location[j]) < self.sensitivity:
                                if self.blocks_color[j] in self._objective_colors:
                                    self._rewards[i] += REWARD_CORRECT_MOVE
                                break
                    
                    # # Punish the agent for moving away the objective
                    # if self._agents_picked_up[i] == -1:
                    #     for j in range(self.n_blocks):
                    #         if np.linalg.norm(self.agents_location[i] - self.blocks_location[j]) < self.sensitivity:
                    #             if self.blocks_color[j] not in self._objective_colors:
                    #                 self._rewards[i] += REWARD_WRONG_MOVE
                    #             break
                    # TODO: penalize collisions with other agents, walls, and blocks
                    # Reward the agent for moving in the right direction while carrying
                    if self._agents_picked_up[i] in self._objective_colors: # If the agent is carrying a objective block
                        target_edge = self.objective[self._objective_colors.index(self._agents_picked_up[i])][1]
                        
                        direction_difference_edge = abs(direction - target_edge) # Calculate the absolute difference
                        direction_difference_edge = min(direction_difference_edge, 360 - direction_difference_edge) # Adjust for cases where the difference is more than 180°
                        direction_difference_edge = max(1 - (direction_difference_edge / 45), 0)  # Normalized to [0, 1]

                        self._rewards[i] += REWARD_CORRECT_MOVE * distance * direction_difference_edge 
        
        self._detect()
        observations = []
        for i in range(self.n_agents):
            observation = self._get_obs(i)

            # for neighbor in observation['neighbors']:
            #     if neighbor[0] in self._objective_colors:
            #         self._rewards[i] +=  REWARD_DETECTING - neighbor[1] # Reward the agent for moving towards the objective
            
            observations.append(observation)
        
        done = False
        # Check if the shared objective is met
        if len(self._completed) == self._task_counter:
            done = True

        reward = sum(self._rewards) # Sum the rewards of all agents of the swarm
        info = {"completed": self._completed}
        truncated = False
        
        return observations, reward, done, truncated, info
    





    no_detections = []
            for o in obs:
                # If it has no neighbors and it is not carrying a block
                no_detections.append(np.all(o['neighbors'][0] == 0) and o['carrying'] == -1)
            
            random_actions = env.action_space.sample()
            if (len(no_detections) - sum(no_detections)) != env.n_agents: # If there is at least one agent with neighbors
                nn_inputs = env.process_observation(obs)
                nn_outputs = np.array(network.predict(nn_inputs))
                actions = np.round(nn_outputs * np.array([env.max_distance_covered_per_step, env.sensor_angle]), 1)
                
                for i in range(env.n_agents):
                    if no_detections[i]:
                        actions[i]= random_actions[i]
            else:
                actions = random_actions

            obs, reward, done, _, _ = env.step(actions)


def _calculate_distance_direction(self, pointA, pointB, distance_type='euclidean', only_distance=False):
        x1, y1 = pointA
        x2, y2 = pointB

        # Calculate distance
        if distance_type == 'manhattan':
            distance = abs(x1 - x2) + abs(y1 - y2)
        elif distance_type == 'euclidean':
            # distance = sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)
            distance = np.linalg.norm(pointA - pointB)
        else:
            raise ValueError("Invalid distance type. Use 'manhattan' or 'euclidean'.")

        # Calculate direction in degrees
        angle_radians = atan2(y2 - y1, x2 - x1)
        direction_degrees = degrees(angle_radians)

        # Normalize the direction to be between 0 and 360 degrees
        if direction_degrees < 0:
            direction_degrees += 360
        # S is 0/360 degrees, E is 90 degrees, N is 180 degrees, W is 270 degrees

        return distance, direction_degrees


        self._rewards = np.zeros(self.n_agents)
        


        # Reset sensors
        self._previous_neighbors = self._neighbors.copy()
        # self._neighbors = np.zeros((self.n_agents, self.n_neighbors, 3), dtype=float)
        
        for i in range(self.n_agents):
            # Get the action of the agent
            distance, direction = action[i]
            if distance > self.max_distance_covered_per_step:
                distance = self.max_distance_covered_per_step
            
            # Calculate the new position
            direction_radians = direction
            if direction > 180:
                direction_radians -= 360
            direction_radians = np.radians(direction)
            dx = round(distance * np.cos(direction_radians), 2)
            dy = round(distance * np.sin(direction_radians), 2)
            # `np.clip` to make sure we don't leave the grid
            new_x = self.agents_location[i][0] + dx
            new_y = self.agents_location[i][1] + dy
            new_position = np.array([new_x, new_y])
            # # Check if it is trying to leave the arena
            # if np.any(new_position < 0) or np.any(new_position >= self.size):
            #     self._rewards[i] += REWARD_WRONG_MOVE # Penalize the agent for crushing into wall
            # Clip values to stay within the grid
            new_position = np.clip(new_position, 0, self.size - 1)
            
            # TODO: this checks or penalize with reward system to avoid these
            # Check if the new position is not too close to another agent
            agent_locations_but_i = np.delete(self.agents_location, i, axis=0)
            differences = new_position - agent_locations_but_i
            distances = np.linalg.norm(differences, axis=1)
            occupied_by_agent = np.any(distances < self.sensitivity)
            # Check if the new position is not too close to a block while carrying one (can't pick up two blocks)
            occupied_by_block_while_carrying = np.any(np.linalg.norm(new_position - self.blocks_location, axis=1) < self.sensitivity) and self._agents_carrying[i] != -1
    


    single_action_space = spaces.Box(low=np.array([0, 0]), 
                                         high=np.array([max_distance_covered_per_step, sensor_angle]), dtype=float)





# Choose the location at random
            for i in range(self.n_agents):
                # Check if the agents are not spawning in the same location
                while True:
                    # Spawn the agents in the nest
                    if self.nest == UP:
                        self.agents_location[i][0] = 0
                        self.agents_location[i][1] = self.np_random.uniform(0, self.size)
                    elif self.nest == DOWN:
                        self.agents_location[i][0] = self.size - 1
                        self.agents_location[i][1] = self.np_random.uniform(0, self.size)
                    elif self.nest == LEFT:
                        self.agents_location[i][0] = self.np_random.uniform(0, self.size)
                        self.agents_location[i][1] = 0
                    elif self.nest == RIGHT:
                        self.agents_location[i][0] = self.np_random.uniform(0, self.size)
                        self.agents_location[i][1] = self.size - 1
                    if i == 0 or not np.any(np.linalg.norm(self.agents_location[i] - self.agents_location[:i], axis=1) < self.sensitivity):
                        break

            for i in range(self.n_blocks - 1):
                # Check if the blocks are not spawning in the same location
                while True:
                    self.blocks_location[i] = self.np_random.uniform(2, self.size - 2, size=2)
                    if i == 0 or not np.any(np.linalg.norm(self.blocks_location[i] - self.blocks_location[:i], axis=1) < self.sensitivity):
                        break
                self.blocks_color[i] = self.np_random.integers(3, 3 + len(self._colors_map), dtype=int)
            # At least one block is objective
            self.blocks_location[self.n_blocks - 1] = self.np_random.uniform(2, self.size - 2, size=2)
            self.blocks_color[self.n_blocks - 1] = self.objective[0][0]



            if initial_setting is not None:
            # TODO: add these checks
            assert 'agents' in initial_setting, "Initial setting must contain 'agents'"
            assert 'blocks' in initial_setting, "Initial setting must contain 'blocks'"
            assert 'colors' in initial_setting, "Initial setting must contain 'colors'"

            self.n_agents = len(initial_setting['agents'])
            self.n_blocks = len(initial_setting['blocks'])
            
            # Check if agents are not in the same location
            for i in range(self.n_agents ):
                for j in range(self.n_agents ):
                    if i != j:
                        assert not np.all(initial_setting['agents'][i] == initial_setting['agents'][j]), f"Agent {i} and agent {j} are spawning in the same location"
            # Check if agents and blocks are not in the same location
            for i in range(self.n_agents ):
                for j in range(self.n_blocks ):
                    assert not np.all(initial_setting['agents'][i] == initial_setting['blocks'][j]), f"Agent {i} and block {j} are spawning in the same location"
            
            # Check if blocks are not spawning in the same location
            for i in range(self.n_blocks ):
                for j in range(self.n_blocks):
                    if i != j:
                        assert not np.all(initial_setting['blocks'][i] == initial_setting['blocks'][j]), f"Block {i} and block {j} are spawning in the same location"
            
            # Check if agents and blocks are within the arena
            for agent in initial_setting['agents']:
                assert agent[0] >= 0 and agent[0] < size, f"Agent {agent} is outside the arena"
                assert agent[1] >= 0 and agent[1] < size, f"Agent {agent} is outside the arena"
            for block in initial_setting['blocks']:
                assert block[0] >= 0 and block[0] < size, f"Block {block} is outside the arena"
                assert block[1] >= 0 and block[1] < size, f"Block {block} is outside the arena"
            
            # All the color objective must be in the blocks colors
            for color, _ in objective:
                assert color in initial_setting['colors'], f"Color {color} in objective is not present in the arena"



    
EASY_INITIAL_SETTING = {
            'agents': np.array([[0, 5], [0, 10], [0, 15]], dtype=float),
            'headings': np.array([DOWN, DOWN, DOWN], dtype=float), 
            'blocks': np.array([[9, 16], [10, 12], [11, 6]], dtype=float),
            'colors': np.array([RED, GREEN, RED], dtype=int)
            }

MEDIUM_INITIAL_SETTING = {
            'agents': np.array([[0, 5], [0, 10], [0, 15]], dtype=float),
            'headings': np.array([DOWN, DOWN, DOWN], dtype=float),
            'blocks': np.array([[9, 16], [11, 5], [6, 5], [10, 11], [8, 8]], dtype=float),
            'colors': np.array([RED, RED, BLUE, GREEN, RED], dtype=int)
            }

HARD_INITIAL_SETTING = {
            'agents': np.array([[0, 3], [0, 6], [0, 9], [0, 12], [0, 15]], dtype=float),
            'headings': np.array([DOWN, DOWN, DOWN, DOWN, DOWN], dtype=float),
            'blocks': np.array([[9, 16], [13, 7], [11, 3], [10, 11], [9, 7], [16, 8], [15, 17],
                               [7, 5], [15, 3], [11, 11], [8, 12], [15, 13], [7, 4]], dtype=float),
            'colors': np.array([RED, RED, RED, RED, RED, RED, RED, RED, BLUE, BLUE, 
                                GREEN, YELLOW, PURPLE], dtype=int)
            }


if self.nest == UP:
                    low = (0, 0)
                    high = (3, SIMULATION_ARENA_SIZE)
                    heading = DOWN
                elif self.nest == DOWN:
                    low = (SIMULATION_ARENA_SIZE - 3, 0)
                    high = (SIMULATION_ARENA_SIZE, SIMULATION_ARENA_SIZE)
                    heading = UP
                elif self.nest == LEFT:
                    low = (0, 0)
                    high = (SIMULATION_ARENA_SIZE, 3)
                    heading = RIGHT
                elif self.nest == RIGHT:
                    low = (0, SIMULATION_ARENA_SIZE - 3)
                    high = (3, SIMULATION_ARENA_SIZE)
                    heading = LEFT


                        def _is_in_drop_zone(self, agent_position):
    def _is_close_to_edge(self, agent_position):
        if agent_position[0] < 1 or agent_position[0] > self.size - 2:
            return True
        if agent_position[1] < 1 or agent_position[1] > self.size - 2:
            return True
        return False

    def _is_correct_drop(self, agent_position, block_idx):
        return agent_position[0] < 1 and self.blocks_color[block_idx] == self.target_color


            # # Handle collisions
        # distances = np.linalg.norm(self.agents_location[:, np.newaxis] - self.agents_location, axis=2)
        # collision_matrix = (distances < self.sensitivity) & (distances > 0)
        # colliding_agents = np.unique(np.where(collision_matrix)[0])
        # if colliding_agents.size > 0:
        #     # self._rewards[colliding_agents] += REWARD_COLLISION  # Penalize collision
        #     # If collision, rotate at max speed
        #     self.agents_location[colliding_agents] = old_agents_location[colliding_agents]
        #     omega_c = self.max_wheel_velocity / (ROBOT_SIZE / 2)
        #     self.agents_heading[colliding_agents] = np.mod(
        #         old_agents_heading[colliding_agents] + np.degrees(omega_c * self.time_step), 360)




#     if self.regularization_retaining == "gd" or self.regularization_retaining == "genetic_distance":
        #             regularization_str = "_gd_"
        #     elif self.regularization_retaining == "wp" or self.regularization_retaining == "weight_protection":
        #             regularization_str = "wp"
        #     if regularization_str == "_gd_":
        #         lambdas_str = f"_{self.reg_lambdas.get('gd')}"
        #     elif regularization_str == "wp":
        #             lambdas_str = f"_{self.reg_lambdas.get('wp')[0]}_{self.reg_lambdas.get('wp')[1]}"
        # else:
        #     regularization_str = ""
        #     lambdas_str = ""